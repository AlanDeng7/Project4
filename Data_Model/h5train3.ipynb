{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20072 images belonging to 6 classes.\n",
      "Found 6752 images belonging to 6 classes.\n",
      "Epoch 1/25\n",
      "628/628 [==============================] - 696s 1s/step - loss: 0.9538 - accuracy: 0.5657 - val_loss: 0.7859 - val_accuracy: 0.6379\n",
      "Epoch 2/25\n",
      "628/628 [==============================] - 689s 1s/step - loss: 0.8188 - accuracy: 0.6345 - val_loss: 0.8383 - val_accuracy: 0.6493\n",
      "Epoch 3/25\n",
      "628/628 [==============================] - 696s 1s/step - loss: 0.7442 - accuracy: 0.6742 - val_loss: 0.8291 - val_accuracy: 0.6379\n",
      "Epoch 4/25\n",
      "628/628 [==============================] - 719s 1s/step - loss: 0.7092 - accuracy: 0.6932 - val_loss: 0.6349 - val_accuracy: 0.7299\n",
      "Epoch 5/25\n",
      "628/628 [==============================] - 719s 1s/step - loss: 0.6545 - accuracy: 0.7203 - val_loss: 0.5800 - val_accuracy: 0.7605\n",
      "Epoch 6/25\n",
      "628/628 [==============================] - 718s 1s/step - loss: 0.6285 - accuracy: 0.7350 - val_loss: 0.5962 - val_accuracy: 0.7559\n",
      "Epoch 7/25\n",
      "628/628 [==============================] - 721s 1s/step - loss: 0.5907 - accuracy: 0.7548 - val_loss: 0.5428 - val_accuracy: 0.7781\n",
      "Epoch 8/25\n",
      "628/628 [==============================] - 719s 1s/step - loss: 0.5625 - accuracy: 0.7650 - val_loss: 0.5707 - val_accuracy: 0.7761\n",
      "Epoch 9/25\n",
      "628/628 [==============================] - 702s 1s/step - loss: 0.5333 - accuracy: 0.7829 - val_loss: 0.4606 - val_accuracy: 0.8186\n",
      "Epoch 10/25\n",
      "628/628 [==============================] - 700s 1s/step - loss: 0.5119 - accuracy: 0.7922 - val_loss: 0.4952 - val_accuracy: 0.8141\n",
      "Epoch 11/25\n",
      "628/628 [==============================] - 715s 1s/step - loss: 0.4959 - accuracy: 0.7997 - val_loss: 0.4461 - val_accuracy: 0.8266\n",
      "Epoch 12/25\n",
      "628/628 [==============================] - 719s 1s/step - loss: 0.4800 - accuracy: 0.8079 - val_loss: 0.4682 - val_accuracy: 0.8128\n",
      "Epoch 13/25\n",
      "628/628 [==============================] - 719s 1s/step - loss: 0.4586 - accuracy: 0.8194 - val_loss: 0.4238 - val_accuracy: 0.8362\n",
      "Epoch 14/25\n",
      "628/628 [==============================] - 719s 1s/step - loss: 0.4439 - accuracy: 0.8236 - val_loss: 0.5079 - val_accuracy: 0.8126\n",
      "Epoch 15/25\n",
      "628/628 [==============================] - 720s 1s/step - loss: 0.4320 - accuracy: 0.8286 - val_loss: 0.4141 - val_accuracy: 0.8368\n",
      "Epoch 16/25\n",
      "628/628 [==============================] - 708s 1s/step - loss: 0.4178 - accuracy: 0.8343 - val_loss: 0.3912 - val_accuracy: 0.8517\n",
      "Epoch 17/25\n",
      "628/628 [==============================] - 699s 1s/step - loss: 0.3932 - accuracy: 0.8445 - val_loss: 0.4047 - val_accuracy: 0.8452\n",
      "Epoch 18/25\n",
      "628/628 [==============================] - 704s 1s/step - loss: 0.3929 - accuracy: 0.8461 - val_loss: 0.3528 - val_accuracy: 0.8663\n",
      "Epoch 19/25\n",
      "628/628 [==============================] - 701s 1s/step - loss: 0.3846 - accuracy: 0.8482 - val_loss: 0.4175 - val_accuracy: 0.8439\n",
      "Epoch 20/25\n",
      "628/628 [==============================] - 715s 1s/step - loss: 0.3904 - accuracy: 0.8456 - val_loss: 0.3392 - val_accuracy: 0.8694\n",
      "Epoch 21/25\n",
      "628/628 [==============================] - 723s 1s/step - loss: 0.3703 - accuracy: 0.8543 - val_loss: 0.3661 - val_accuracy: 0.8621\n",
      "Epoch 22/25\n",
      "628/628 [==============================] - 737s 1s/step - loss: 0.3580 - accuracy: 0.8606 - val_loss: 0.3999 - val_accuracy: 0.8528\n",
      "Epoch 23/25\n",
      "628/628 [==============================] - 819s 1s/step - loss: 0.3484 - accuracy: 0.8647 - val_loss: 0.3700 - val_accuracy: 0.8646\n",
      "Epoch 24/25\n",
      "628/628 [==============================] - 748s 1s/step - loss: 0.3456 - accuracy: 0.8658 - val_loss: 0.4159 - val_accuracy: 0.8491\n",
      "Epoch 25/25\n",
      "628/628 [==============================] - 721s 1s/step - loss: 0.3411 - accuracy: 0.8666 - val_loss: 0.3045 - val_accuracy: 0.8886\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Define data directories\n",
    "train_dir = 'Q:/CODE/Project4/Test_train_sets/train2'\n",
    "test_dir = 'Q:/CODE/Project4/Test_train_sets/test2'\n",
    "\n",
    "# Image properties\n",
    "IMG_HEIGHT, IMG_WIDTH = 240, 240\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Preprocess and augment the training data using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "# Just rescale the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "# Flow validation images in batches using test_datagen generator\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # 6 classes of animals\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=test_generator, epochs=25)\n",
    "\n",
    "# Define the path to save the model in your Google Drive\n",
    "save_path = 'Q:/CODE/Project4/animal_classifierv3.h5'\n",
    "\n",
    "# Save the model to the specified path in your Google Drive\n",
    "model.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
